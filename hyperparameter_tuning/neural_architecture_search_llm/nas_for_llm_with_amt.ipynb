{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Architecture Search for Large Language Models\n",
    "\n",
    "Neural Architecture Search (NAS) is an effective framework to design neural network architectures automatically.\n",
    "This notebook demonstrates how to utilize NAS for compressing a large language model that has been fine-tuned for a specific target task. The main objective is to reduce the model size while maintaining performance as much as possible. To achieve this, we search for sub-networks within the model that jointly optimize the parameter count and validation error.\n",
    "\n",
    "Sub-networks can be defined in various ways. In this context, we consider subsets of multi-head attention and fully-connected layers, with a reduced number of heads in the multi-head attention layer and units in the intermediate layers.\n",
    "\n",
    "Our NAS approach consists of two steps:\n",
    "\n",
    "1. Initial fine-tuning of the pre-trained model on the target task using weight-sharing based NAS training strategies. The pre-trained model serves as a 'super-network' that encompasses a large, finite set of sub-networks. To prevent sub-networks from co-adapting, we modify the fine-tuning process by updating only specific parts of the network (i.e., subsets of all layers) in each update step.\n",
    "\n",
    "2. In the second step, we employ multi-objective search through [SageMaker Automated Model Tuning (AMT)](https://aws.amazon.com/sagemaker/automatic-model-tuning/) to identify a set of sub-networks that offer an optimal trade-off between parameter count and validation error for the target task.\n",
    "\n",
    "Finally, we can visualize the so-called **Pareto set** of architectures that optimally balance between parameter count and validation error, and select the best-suited model that strikes the right balance between model size and validation error for us.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "- Currently, the notebook limited to the BERT model family.\n",
    "- We exclusively focus on supervised fine-tuning using labeled datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Install requirements\n",
    "\n",
    "Before we get started, we install all requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "from training import train_supernetwork\n",
    "from estimate_efficency import compute_latency\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "We define all hyperparameters and model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_type = \"bert-base-cased\"\n",
    "output_dir = \"nas_output_dir\"\n",
    "max_seq_length = 128\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Dataset and Evaluation Metric\n",
    "\n",
    "For illustration purposes, we use the [Recognizing Textual Entailment](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) dataset from the [GLUE](https://gluebenchmark.com/) benchmarking suite. The goal of this dataset is to identify whether the meaning of one sentence can be inferred from the other sentence. \n",
    "Example sentence pair:\n",
    " \n",
    "sentence 1:\n",
    "*Only a week after it had no comment on upping the storage capacity of its Hotmail e-mail service, Microsoft early Thursday announced it was boosting the allowance to 250 MB to follow similar moves by rivals such as Google, Yahoo, and Lycos.*\n",
    "\n",
    "sentence 2:\n",
    "*Microsoft's Hotmail has raised its storage capacity to 250 MB.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We load the dataset via the [dataset library](https://huggingface.co/docs/datasets/index) from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_name = \"rte\"\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", task_name)\n",
    "sentence1_key, sentence2_key = (\"sentence1\", \"sentence2\")\n",
    "\n",
    "metric = evaluate.load(\"glue\", task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We first preprocess the data using the Tokenizer from HuggingFace. This code is originally from this [example]( https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "padding = \"max_length\"\n",
    "\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    args = (\n",
    "        (examples[sentence1_key],)\n",
    "        if sentence2_key is None\n",
    "        else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(\n",
    "        *args, padding=padding, max_length=max_seq_length, truncation=True\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "preproc_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "label_list = preproc_datasets[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Split Dataset in Training / Validation\n",
    "\n",
    "As mentioned earlier, after fine-tuning our 'super-network' on the training dataset, we proceed with a multi-objective search to identify the optimal set of sub-networks that optimally balance between generalization performance and parameter count. Since we cannot directly compute generalization performance, we approximate it by measuring accuracy on a hold-out validation dataset.\n",
    "\n",
    "To achieve this, we split the original training dataset from GLUE into a training/validation set. The training set is exclusively used for fine-tuning purposes, while the validation data is employed for the subsequent multi-objective search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = preproc_datasets[\"train\"]\n",
    "test_dataset = preproc_datasets[\n",
    "    \"validation_matched\" if task_name == \"mnli\" else \"validation\"\n",
    "]\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"idx\"])\n",
    "test_dataset = test_dataset.remove_columns([\"idx\"])\n",
    "\n",
    "split = train_dataset.train_test_split(train_size=0.7, seed=seed)\n",
    "train_dataset = split[\"train\"]\n",
    "valid_dataset = split[\"test\"]\n",
    "\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=per_device_train_batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=per_device_eval_batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load pre-trained Model\n",
    "\n",
    "Load the [pre-trained BERT model](https://huggingface.co/bert-base-cased) from the HuggingFace hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_type,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=task_name,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_type,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train weight-sharing based Super-Network\n",
    "\n",
    "We now fine-tune our pre-training network, i.e. super-network. To control the training we can use the ['TrainingArguments'](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) of the HuggingFace transformer library. We have to pass an additional argument to specify if our dataset is regression or not (determines the loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=output_dir)\n",
    "training_args.is_regression = (\n",
    "    False  # set this to True if your dataset is a regression dataset, for example STSB\n",
    ")\n",
    "training_args.save_strategy = \"epoch\"\n",
    "training_args.log_dir = \".log_dir\"\n",
    "training_args.seed = seed\n",
    "training_args.use_accelerate = False\n",
    "training_args.num_train_epochs = 10\n",
    "training_args.learning_rate = 2e-05\n",
    "training_args.per_device_train_batch_size = per_device_train_batch_size\n",
    "training_args.per_device_eval_batch_size = per_device_eval_batch_size\n",
    "train_supernetwork(model, train_dataloader, eval_dataloader, metric, training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multi-objective search for sub-networks\n",
    "\n",
    "After the fine-tuning process, we begin the multi-objective search by sampling random sub-networks using AMT. A sub-network is defined by its number of layers, heads, and units in the intermediate fully connected layers. To access a sub-network, we place a binary mask over the super-network and mask out all components (i.e., heads, units) that are not part of the sub-network. Note that, HuggingFace transformers needs the hidden size to be a multiple of the number of head. We cannot change the hidden size, and hence the number of heads has to be in [1, 3, 6, 12].\n",
    "\n",
    "In contrast to single-objective optimization, in the multi-objective setting, we typically do not have a single solution that simultaneously optimizes all objectives. Instead, we aim to collect a set of solutions that *dominate* all other solutions in at least one objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AMT spawns multiple SageMaker training jobs to evaluate sub-networks. To allows these training jobs to access the checkpoint of our model, we have to first upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def upload_to_s3(src_dir):\n",
    "    session = Session()\n",
    "    s3_bucket = session.default_bucket()\n",
    "    s3_bucket_prefix = \"nas_amt/model_checkpoint\"\n",
    "\n",
    "    s3_path = f\"s3://{s3_bucket}/{s3_bucket_prefix}\"\n",
    "\n",
    "    cmd = f\"aws s3 sync {src_dir} {s3_path}\"\n",
    "    logging.info(f\"upload results to {s3_path} with command {cmd}\")\n",
    "    subprocess.run(cmd.split(\" \"))\n",
    "\n",
    "    return s3_path\n",
    "\n",
    "\n",
    "s3_path = upload_to_s3(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can start the multi-objective search through AMT. We sample a total of 100 random sub-networks (defined by the parameter `max_jobs`) and evaluate 10 networks simultaneously (defined by `max_parallel_jobs`). The code to load the model checkpoint and evaluate the sub-network is available in the `evaluate_subnetwork.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Maximum number of sub-networks we will evaluate\n",
    "max_jobs = 100\n",
    "max_parallel_jobs = 10\n",
    "\n",
    "# Entry point script to load the super-network and evaluate a sub-network\n",
    "entry_point = \"evaluate_subnetwork.py\"\n",
    "\n",
    "# Command line arguments for the entry point script\n",
    "hyperparameters = {\n",
    "    \"model_name_or_path\": model_type,\n",
    "    \"output_dir\": \"./tmp\",\n",
    "    \"task_name\": \"rte\",\n",
    "}\n",
    "\n",
    "# Define the metric we want to maximize\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"num-parameters\", \"Regex\": \"number of parameters: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"validation-performance\", \"Regex\": \"validation score: ([0-9\\\\.]+)\"},\n",
    "]\n",
    "\n",
    "# Define HuggingFace estimator\n",
    "estimator = HuggingFace(\n",
    "    entry_point=entry_point,\n",
    "    source_dir=\"./\",\n",
    "    instance_type=\"ml.g4dn.xlarge\",  # instance types for the SageMaker training jobs\n",
    "    instance_count=1,\n",
    "    py_version=\"py39\",\n",
    "    framework_version=\"1.13\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    transformers_version=\"4.26\",\n",
    "    max_run=3600 * 72,\n",
    "    role=get_execution_role(),\n",
    "    volume_size=125,\n",
    "    model_uri=s3_path,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "\n",
    "current_time = datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "tuning_job_name = f\"nas-search-{current_time}\"\n",
    "\n",
    "# Search space to define sub-networks\n",
    "hyperparameter_ranges = {\n",
    "    \"num_layers\": IntegerParameter(0, 12),\n",
    "    # To meet HuggingFace constraints, we can only set the number of head to these values\n",
    "    \"num_heads\": CategoricalParameter([1, 3, 6, 12]),\n",
    "    \"num_units\": IntegerParameter(0, 3072),\n",
    "}\n",
    "\n",
    "# Define AMT Tuner object\n",
    "my_tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name=\"validation-performance\",\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_jobs=max_jobs,\n",
    "    strategy=\"Random\",\n",
    "    random_seed=seed,\n",
    "    max_parallel_jobs=max_parallel_jobs,\n",
    ")\n",
    "\n",
    "# Start hyperparameter tuning job\n",
    "my_tuner.fit(job_name=tuning_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualize Results\n",
    "\n",
    "To visualize our results, we parse AMT's history to collect all configurations of sub-networks and the corresponding metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = my_tuner.analytics().dataframe()\n",
    "data = []\n",
    "configs = []\n",
    "for i, t in enumerate(my_tuner.analytics().training_job_summaries()):\n",
    "    jn = t[\"TrainingJobName\"]\n",
    "    df = sagemaker.analytics.TrainingJobAnalytics(jn).dataframe()\n",
    "\n",
    "    row = history[history[\"TrainingJobName\"] == jn]\n",
    "    config = {\n",
    "        \"num_heads\": int(row[\"num_heads\"].iloc[0].strip('\"')),\n",
    "        \"num_layers\": int(row[\"num_layers\"]),\n",
    "        \"num_units\": int(row[\"num_units\"]),\n",
    "    }\n",
    "    configs.append(config)\n",
    "\n",
    "    p = []\n",
    "    for j, metric in enumerate(metric_definitions):\n",
    "        metric_name = metric[\"Name\"]\n",
    "        if \"metric_name\" not in df.keys():\n",
    "            continue\n",
    "\n",
    "        if metric_name == \"validation-performance\":\n",
    "            y = 1 - float(df[df[\"metric_name\"] == metric_name][\"value\"])\n",
    "        else:\n",
    "            y = float(df[df[\"metric_name\"] == metric_name][\"value\"])\n",
    "        p.append(y)\n",
    "    if len(p) > 0:\n",
    "        data.append(p)\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Just for comparison, we also evaluate the un-pruned network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\")\n",
    "metric = load(\"glue\", task_name)\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "eval_metric = metric.compute()\n",
    "validation_error_unpruned_network = 1 - eval_metric[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now visualize the Pareto set, which represents the optimal set of sub-networks that dominate all other sub-networks in at least one metric. This implies that when we move from one sub-network of the Pareto set to another, we must either sacrifice performance or model size but improve the other.\n",
    "\n",
    "Ultimately, the Pareto set provides us the flexibility to choose the sub-network that best suits our preferences. We can decide how much we want to reduce the size of our network and how much performance we are willing to sacrifice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from multi_objective import get_pareto_optimal\n",
    "\n",
    "n_params_original_model = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "plt.scatter(\n",
    "    n_params_original_model,\n",
    "    validation_error_unpruned_network,\n",
    "    color=\"red\",\n",
    "    marker=\"x\",\n",
    "    s=80,\n",
    "    label=\"unpruned network\",\n",
    ")\n",
    "\n",
    "\n",
    "idx = get_pareto_optimal(data)\n",
    "x = data[idx, 0]\n",
    "y = data[idx, 1]\n",
    "\n",
    "plt.scatter(\n",
    "    x,\n",
    "    y,\n",
    "    marker=\"o\",\n",
    "    s=80,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"C0\",\n",
    "    linewidth=2,\n",
    "    label=\"Pareto front\",\n",
    ")\n",
    "plt.xlabel(\"number of parameters\")\n",
    "plt.ylabel(\"validation error\")\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(linewidth=\"1\", alpha=0.4, which=\"both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Select Final Model\n",
    "\n",
    "To choose the final sub-network, we create a new model with a new architecture definition and then copy the subset of weights from the super-network into this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertConfig\n",
    "from extract_subnetworks import get_final_bert_model\n",
    "\n",
    "# Select the largest sub-network of the Pareto front (lower right figure above)\n",
    "best = np.arange(data.shape[0])[idx][-1]\n",
    "architecture_definition = configs[best]\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=model.config.vocab_size,\n",
    "    num_hidden_layers=architecture_definition[\"num_layers\"],\n",
    "    num_attention_heads=architecture_definition[\"num_heads\"],\n",
    "    intermediate_size=architecture_definition[\"num_units\"],\n",
    ")\n",
    "config.attention_head_size = int(config.hidden_size / model.config.num_attention_heads)\n",
    "\n",
    "new_model = get_final_bert_model(original_model=model, new_model_config=config)\n",
    "\n",
    "n_params_full_model = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "n_params_new_model = sum(p.numel() for p in new_model.parameters() if p.requires_grad)\n",
    "print(f\"Params full model = {n_params_full_model}\")\n",
    "print(f\"Params new model = {n_params_new_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency full model = 10.527280648549398\n",
      "Latency new model = 9.942786769866943\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "batch = train_dataset[0][sentence1_key]\n",
    "latency = compute_latency(model, tokenizer, batch, device)\n",
    "print(f\"Latency full model = {latency}\")\n",
    "new_model.to(device)\n",
    "latency = compute_latency(new_model, tokenizer, batch, device)\n",
    "print(f\"Latency new model = {latency}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

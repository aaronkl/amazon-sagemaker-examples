{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Architecture Search for Large Language Models\n",
    "\n",
    "Neural Architecture Search (NAS) is an effective framework to design neural network architectures automatically.\n",
    "This notebook demonstrates how to utilize NAS for compressing a large language model that has been fine-tuned for a specific target task. The main objective is to reduce the model size while maintaining performance as much as possible. To achieve this, we search for sub-networks within the model that jointly optimize the parameter count and validation error.\n",
    "\n",
    "Sub-networks can be defined in various ways. In this context, we consider subsets of multi-head attention and fully-connected layers, with a reduced number of heads in the multi-head attention layer and units in the intermediate layers.\n",
    "\n",
    "Our NAS approach consists of two steps:\n",
    "\n",
    "1. Initial fine-tuning of the pre-trained model on the target task using weight-sharing based NAS training strategies. The pre-trained model serves as a 'super-network' that encompasses a large, finite set of sub-networks. To prevent sub-networks from co-adapting, we modify the fine-tuning process by updating only specific parts of the network (i.e., subsets of all layers) in each update step.\n",
    "\n",
    "2. In the second step, we employ multi-objective search through [SageMaker Automated Model Tuning (AMT)](https://aws.amazon.com/sagemaker/automatic-model-tuning/) to identify a set of sub-networks that offer an optimal trade-off between parameter count and validation error for the target task.\n",
    "\n",
    "Finally, we can visualize the so-called **Pareto set** of architectures that optimally balance between parameter count and validation error, and select the best-suited model that strikes the right balance between model size and validation error for us.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "- Currently, the notebook limited to the BERT model family.\n",
    "- We exclusively focus on supervised fine-tuning using labeled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter\n",
    "\n",
    "# from sagemaker import get_execution_role\n",
    "from syne_tune.backend.sagemaker_backend.sagemaker_utils import get_execution_role\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset \n",
    "\n",
    "For illustration purposes, we use the [Recognizing Textual Entailment](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) dataset from the [GLUE](https://gluebenchmark.com/) benchmarking suite. For simplicity, we load the dataset via the [dataset library](https://huggingface.co/docs/datasets/index) from HuggingFace inside our training script.\n",
    "The goal of this dataset is to identify whether the meaning of one sentence can be infered from the other sentence. \n",
    "Example sentence pair:\n",
    " \n",
    "sentence 1:\n",
    "*Only a week after it had no comment on upping the storage capacity of its Hotmail e-mail service, Microsoft early Thursday announced it was boosting the allowance to 250 MB to follow similar moves by rivals such as Google, Yahoo, and Lycos.*\n",
    "\n",
    "sentence 2:\n",
    "*Microsoft's Hotmail has raised its storage capacity to 250 MB.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "hyperparameters = dict()\n",
    "\n",
    "hyperparameters[\"seed\"] = seed\n",
    "hyperparameters[\"output_dir\"] = \"/opt/ml/checkpoints\"\n",
    "hyperparameters[\"task_name\"] = \"rte\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Split Dataset in Training / Validation\n",
    "\n",
    "As mentioned earlier, after fine-tuning our 'super-network' on the training dataset, we proceed with a multi-objective search to identify the optimal set of sub-networks that optimally balance between generalization performance and parameter count. Since we cannot directly compute generalization performance, we approximate it by measuring accuracy on a hold-out validation dataset.\n",
    "\n",
    "To achieve this, we split the original training dataset from GLUE into a training/validation set. The training set is exclusively used for fine-tuning purposes, while the validation data is employed for the subsequent multi-objective search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pre-trained Model\n",
    "\n",
    "We use the [pre-trained BERT model](https://huggingface.co/bert-base-cased) from the HuggingFace hub, which consists of 12 layers with 12 heads in each multi-head attention layers and 3072 units in the fully connected layers. See [Devlin et al.](https://aclanthology.org/N19-1423/) for a more detailed description of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"bert-base-cased\"\n",
    "hyperparameters[\"model_name_or_path\"] = model_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train weight-sharing based Super-Network\n",
    "\n",
    "We now fine-tune our pre-training network, i.e. super-network with the following hyperparameters. We have to pass an additional argument to specify if our dataset is regression or not (determines the loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparameters[\"per_device_train_batch_size\"] = 8\n",
    "hyperparameters[\"per_device_eval_batch_size\"] = 8\n",
    "hyperparameters[\"learning_rate\"] = 2e-05\n",
    "hyperparameters[\"num_train_epochs\"] = 5\n",
    "hyperparameters[\"save_strategy\"] = \"epoch\"\n",
    "hyperparameters[\n",
    "    \"is_regression\"\n",
    "] = False  # set this to True if your dataset is a regression dataset, for example STSB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the checkpoint of the model on S3, such that we can load it later in the second phase for the multi-objective search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-08-04 15:24:18 Starting - Starting the training job...\n",
      "2023-08-04 15:24:33 Starting - Preparing the instances for training......\n",
      "2023-08-04 15:25:39 Downloading - Downloading input data......\n",
      "2023-08-04 15:26:30 Training - Downloading the training image............\n",
      "2023-08-04 15:29:11 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:16,394 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:16,416 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:16,428 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:16,431 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:17,693 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/aaronkl/transformers.git (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/aaronkl/transformers.git to /tmp/pip-req-build-kdw8jso8\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/aaronkl/transformers.git /tmp/pip-req-build-kdw8jso8\u001b[0m\n",
      "\u001b[34mResolved https://github.com/aaronkl/transformers.git to commit a40386669f2b4c147439bb4370cecd7b764ace8a\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 4.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->-r requirements.txt (line 5)) (4.7.1)\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.0.0 (from evaluate->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets>=2.0.0 from https://files.pythonhosted.org/packages/25/55/ec0d602cec473f3857ca82c9f2ddbd5b8c4d1139debbf06e19aaff29f973/datasets-2.14.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.3-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 6)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 6)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from evaluate->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/45/63/40da996350689cf29db7f8819aafa74c9d36feca4f0e4393d220c619a1dc/xxhash-3.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 6)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 6)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.7.0 (from evaluate->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub>=0.7.0 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 6)) (23.1)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0.dev0->-r requirements.txt (line 7)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0.dev0->-r requirements.txt (line 7)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.24.0.dev0->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/5e/a8/2e3626392c4fcf7e3920cae166155542838be2048384989e2c6f024b793d/regex-2023.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 11.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.24.0.dev0->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 72.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 6)) (12.0.1)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets>=2.0.0->evaluate->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/5b/8d/821fcb268cfc056964a75da3823896b17eabaa4968a2414121bc93b0c501/aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 6)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 6)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 6)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 6)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 6)) (23.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 23.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 269.4/269.4 kB 49.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/b5/03/7dec2e257bd173b5ca1f74477863b97d322149f6f0284d7decead8c5ceeb/frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.3-py3-none-any.whl (519 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.1/519.1 kB 60.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 44.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 769.9/769.9 kB 75.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.8/193.8 kB 37.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 80.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.0/228.0 kB 50.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transformers\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for transformers: filename=transformers-4.24.0.dev0-py3-none-any.whl size=5419880 sha256=1c728f92ed73046916a79d9d5e73b3b798b830416dc342c002107755201cbf9e\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-js_xudzw/wheels/20/07/29/0cc93aaae6b36fb6655bd9d0427bc9f8a3ba3108e06f5d30da\u001b[0m\n",
      "\u001b[34mSuccessfully built transformers\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mInstalling collected packages: tokenizers, xxhash, regex, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.14.3 evaluate-0.4.0 frozenlist-1.4.0 huggingface-hub-0.16.4 multidict-6.0.4 regex-2023.6.3 responses-0.18.0 tokenizers-0.13.3 transformers-4.24.0.dev0 xxhash-3.3.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:38,581 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:38,581 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:38,629 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:38,672 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:38,716 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:38,733 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"is_regression\": false,\n",
      "        \"learning_rate\": 2e-05,\n",
      "        \"model_name_or_path\": \"bert-base-cased\",\n",
      "        \"num_train_epochs\": 5,\n",
      "        \"output_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"per_device_eval_batch_size\": 8,\n",
      "        \"per_device_train_batch_size\": 8,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"seed\": \"seed\",\n",
      "        \"task_name\": \"rte\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-08-04-15-24-14-265\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-770209394645/pytorch-training-2023-08-04-15-24-14-265/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"training\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"training.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"is_regression\":false,\"learning_rate\":2e-05,\"model_name_or_path\":\"bert-base-cased\",\"num_train_epochs\":5,\"output_dir\":\"/opt/ml/checkpoints\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"save_strategy\":\"epoch\",\"seed\":\"seed\",\"task_name\":\"rte\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=training.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=training\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-770209394645/pytorch-training-2023-08-04-15-24-14-265/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"is_regression\":false,\"learning_rate\":2e-05,\"model_name_or_path\":\"bert-base-cased\",\"num_train_epochs\":5,\"output_dir\":\"/opt/ml/checkpoints\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"save_strategy\":\"epoch\",\"seed\":\"seed\",\"task_name\":\"rte\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-08-04-15-24-14-265\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-770209394645/pytorch-training-2023-08-04-15-24-14-265/source/sourcedir.tar.gz\",\"module_name\":\"training\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"training.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--is_regression\",\"False\",\"--learning_rate\",\"2e-05\",\"--model_name_or_path\",\"bert-base-cased\",\"--num_train_epochs\",\"5\",\"--output_dir\",\"/opt/ml/checkpoints\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"8\",\"--save_strategy\",\"epoch\",\"--seed\",\"seed\",\"--task_name\",\"rte\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_IS_REGRESSION=false\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=bert-base-cased\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=seed\u001b[0m\n",
      "\u001b[34mSM_HP_TASK_NAME=rte\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 training.py --is_regression False --learning_rate 2e-05 --model_name_or_path bert-base-cased --num_train_epochs 5 --output_dir /opt/ml/checkpoints --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --save_strategy epoch --seed seed --task_name rte\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:38,765 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34musage: training.py [-h] --model_name_or_path MODEL_NAME_OR_PATH\n",
      "                   [--cache_dir CACHE_DIR]\n",
      "                   [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n",
      "                   [--no_use_fast_tokenizer] [--model_revision MODEL_REVISION]\n",
      "                   [--use_auth_token [USE_AUTH_TOKEN]] [--task_name TASK_NAME]\n",
      "                   [--max_seq_length MAX_SEQ_LENGTH]\n",
      "                   [--overwrite_cache [OVERWRITE_CACHE]]\n",
      "                   [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n",
      "                   [--no_pad_to_max_length] [--is_regression [IS_REGRESSION]]\n",
      "                   --output_dir OUTPUT_DIR\n",
      "                   [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
      "                   [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
      "                   [--do_predict [DO_PREDICT]]\n",
      "                   [--evaluation_strategy {no,steps,epoch}]\n",
      "                   [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
      "                   [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                   [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                   [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                   [--eval_delay EVAL_DELAY] [--learning_rate LEARNING_RATE]\n",
      "                   [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
      "                   [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
      "                   [--max_grad_norm MAX_GRAD_NORM]\n",
      "                   [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                   [--max_steps MAX_STEPS]\n",
      "                   [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
      "                   [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]\n",
      "                   [--log_level {debug,info,warning,error,critical,passive}]\n",
      "                   [--log_level_replica {debug,info,warning,error,critical,passive}]\n",
      "                   [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
      "                   [--no_log_on_each_node] [--logging_dir LOGGING_DIR]\n",
      "                   [--logging_strategy {no,steps,epoch}]\n",
      "                   [--logging_first_step [LOGGING_FIRST_STEP]]\n",
      "                   [--logging_steps LOGGING_STEPS]\n",
      "                   [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n",
      "                   [--no_logging_nan_inf_filter]\n",
      "                   [--save_strategy {no,steps,epoch}]\n",
      "                   [--save_steps SAVE_STEPS]\n",
      "                   [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                   [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
      "                   [--no_cuda [NO_CUDA]] [--use_mps_device [USE_MPS_DEVICE]]\n",
      "                   [--seed SEED] [--data_seed DATA_SEED]\n",
      "                   [--jit_mode_eval [JIT_MODE_EVAL]] [--use_ipex [USE_IPEX]]\n",
      "                   [--bf16 [BF16]] [--fp16 [FP16]]\n",
      "                   [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                   [--half_precision_backend {auto,cuda_amp,apex,cpu_amp}]\n",
      "                   [--bf16_full_eval [BF16_FULL_EVAL]]\n",
      "                   [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\n",
      "                   [--local_rank LOCAL_RANK] [--xpu_backend {mpi,ccl,gloo}]\n",
      "                   [--tpu_num_cores TPU_NUM_CORES]\n",
      "                   [--tpu_metrics_debug [TPU_METRICS_DEBUG]] [--debug DEBUG]\n",
      "                   [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
      "                   [--eval_steps EVAL_STEPS]\n",
      "                   [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                   [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                   [--disable_tqdm DISABLE_TQDM]\n",
      "                   [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
      "                   [--no_remove_unused_columns]\n",
      "                   [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "                   [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
      "                   [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "                   [--greater_is_better GREATER_IS_BETTER]\n",
      "                   [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
      "                   [--sharded_ddp SHARDED_DDP] [--fsdp FSDP]\n",
      "                   [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
      "                   [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
      "                   [--deepspeed DEEPSPEED]\n",
      "                   [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                   [--optim {adamw_hf,adamw_torch,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_bnb_8bit,sgd,adagrad}]\n",
      "                   [--adafactor [ADAFACTOR]]\n",
      "                   [--group_by_length [GROUP_BY_LENGTH]]\n",
      "                   [--length_column_name LENGTH_COLUMN_NAME]\n",
      "                   [--report_to REPORT_TO [REPORT_TO ...]]\n",
      "                   [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
      "                   [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
      "                   [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
      "                   [--no_dataloader_pin_memory]\n",
      "                   [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
      "                   [--no_skip_memory_metrics]\n",
      "                   [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
      "                   [--push_to_hub [PUSH_TO_HUB]]\n",
      "                   [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                   [--hub_model_id HUB_MODEL_ID]\n",
      "                   [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
      "                   [--hub_token HUB_TOKEN]\n",
      "                   [--hub_private_repo [HUB_PRIVATE_REPO]]\n",
      "                   [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
      "                   [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n",
      "                   [--fp16_backend {auto,cuda_amp,apex,cpu_amp}]\n",
      "                   [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
      "                   [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
      "                   [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
      "                   [--mp_parameters MP_PARAMETERS]\n",
      "                   [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n",
      "                   [--full_determinism [FULL_DETERMINISM]]\n",
      "                   [--torchdynamo {eager,nvfuser,fx2trt,fx2trt-fp16}]\n",
      "                   [--ray_scope RAY_SCOPE] [--ddp_timeout DDP_TIMEOUT]\u001b[0m\n",
      "\u001b[34mtraining.py: error: argument --seed: invalid int value: 'seed'\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:41,567 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:41,567 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 2 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:41,568 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:41,568 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 2\u001b[0m\n",
      "\u001b[34mErrorMessage \"\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.9 training.py --is_regression False --learning_rate 2e-05 --model_name_or_path bert-base-cased --num_train_epochs 5 --output_dir /opt/ml/checkpoints --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --save_strategy epoch --seed seed --task_name rte\"\u001b[0m\n",
      "\u001b[34m2023-08-04 15:29:41,568 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-08-04 15:29:57 Uploading - Uploading generated training model\n",
      "2023-08-04 15:29:57 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2023-08-04-15-24-14-265: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 2\nErrorMessage \"\"\nCommand \"/opt/conda/bin/python3.9 training.py --is_regression False --learning_rate 2e-05 --model_name_or_path bert-base-cased --num_train_epochs 5 --output_dir /opt/ml/checkpoints --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --save_strategy epoch --seed seed --task_name rte\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 22\u001b[0m\n\u001b[1;32m      6\u001b[0m sm_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m      7\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining.py\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     source_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     checkpoint_s3_uri\u001b[38;5;241m=\u001b[39ms3_path,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m est \u001b[38;5;241m=\u001b[39m PyTorch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msm_args)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv3.10/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:300\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv3.10/lib/python3.10/site-packages/sagemaker/estimator.py:1252\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv3.10/lib/python3.10/site-packages/sagemaker/estimator.py:2398\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2396\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv3.10/lib/python3.10/site-packages/sagemaker/session.py:4761\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4741\u001b[0m     \u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4742\u001b[0m \n\u001b[1;32m   4743\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4759\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4760\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4761\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv3.10/lib/python3.10/site-packages/sagemaker/session.py:6630\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6627\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   6629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 6630\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   6632\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/venv3.10/lib/python3.10/site-packages/sagemaker/session.py:6683\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   6678\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   6679\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6680\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6681\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6682\u001b[0m     )\n\u001b[0;32m-> 6683\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6684\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6685\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6686\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6687\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2023-08-04-15-24-14-265: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 2\nErrorMessage \"\"\nCommand \"/opt/conda/bin/python3.9 training.py --is_regression False --learning_rate 2e-05 --model_name_or_path bert-base-cased --num_train_epochs 5 --output_dir /opt/ml/checkpoints --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --save_strategy epoch --seed seed --task_name rte\", exit code: 1"
     ]
    }
   ],
   "source": [
    "session = Session()\n",
    "s3_bucket = session.default_bucket()\n",
    "s3_bucket_prefix = \"nas_amt/model_checkpoint\"\n",
    "s3_path = f\"s3://{s3_bucket}/{s3_bucket_prefix}\"\n",
    "\n",
    "sm_args = dict(\n",
    "    entry_point=\"training.py\",\n",
    "    source_dir=os.path.abspath(\"\"),\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    instance_count=1,\n",
    "    py_version=\"py39\",\n",
    "    framework_version=\"1.12\",\n",
    "    max_run=3600 * 72,\n",
    "    role=get_execution_role(),\n",
    "    checkpoint_local_path=\"/opt/ml/checkpoints\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    checkpoint_s3_uri=s3_path,\n",
    ")\n",
    "\n",
    "est = PyTorch(**sm_args)\n",
    "est.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multi-objective search for sub-networks\n",
    "\n",
    "After the fine-tuning process, we begin the multi-objective search by sampling random sub-networks using AMT. A sub-network is defined by its number of layers, heads, and units in the intermediate fully connected layers. To access a sub-network, we place a binary mask over the super-network and mask out all components (i.e., heads, units) that are not part of the sub-network. Note that, HuggingFace transformers needs the hidden size to be a multiple of the number of head. We cannot change the hidden size, and hence the number of heads has to be in [1, 3, 6, 12].\n",
    "\n",
    "In contrast to single-objective optimization, in the multi-objective setting, we typically do not have a single solution that simultaneously optimizes all objectives. Instead, we aim to collect a set of solutions that *dominate* all other solutions in at least one objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can start the multi-objective search through AMT. We sample a total of 100 random sub-networks (defined by the parameter `max_jobs`) and evaluate 10 networks simultaneously (defined by `max_parallel_jobs`). The code to load the model checkpoint and evaluate the sub-network is available in the `evaluate_subnetwork.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Maximum number of sub-networks we will evaluate\n",
    "max_jobs = 100\n",
    "max_parallel_jobs = 10\n",
    "\n",
    "# Entry point script to load the super-network and evaluate a sub-network\n",
    "entry_point = \"evaluate_subnetwork.py\"\n",
    "\n",
    "# Command line arguments for the entry point script\n",
    "hyperparameters = {\"model_name_or_path\": model_type, \"output_dir\": \"./tmp\", \"task_name\": \"rte\"}\n",
    "\n",
    "# Define the metric we want to maximize\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"num-parameters\", \"Regex\": \"number of parameters: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"validation-performance\", \"Regex\": \"validation score: ([0-9\\\\.]+)\"},\n",
    "]\n",
    "\n",
    "# Define HuggingFace estimator\n",
    "estimator = HuggingFace(\n",
    "    entry_point=entry_point,\n",
    "    source_dir=\"./\",\n",
    "    instance_type=\"ml.g4dn.xlarge\",  # instance types for the SageMaker training jobs\n",
    "    instance_count=1,\n",
    "    py_version=\"py39\",\n",
    "    framework_version=\"1.13\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    transformers_version=\"4.26\",\n",
    "    max_run=3600 * 72,\n",
    "    role=get_execution_role(),\n",
    "    volume_size=125,\n",
    "    model_uri=s3_path,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "\n",
    "current_time = datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "tuning_job_name = f\"nas-search-{current_time}\"\n",
    "\n",
    "# Search space to define sub-networks\n",
    "hyperparameter_ranges = {\n",
    "    \"num_layers\": IntegerParameter(0, 12),\n",
    "    # To meet HuggingFace constraints, we can only set the number of head to these values\n",
    "    \"num_heads\": CategoricalParameter([1, 3, 6, 12]),\n",
    "    \"num_units\": IntegerParameter(0, 3072),\n",
    "}\n",
    "\n",
    "# Define AMT Tuner object\n",
    "my_tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name=\"validation-performance\",\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_jobs=max_jobs,\n",
    "    strategy=\"Random\",\n",
    "    random_seed=seed,\n",
    "    max_parallel_jobs=max_parallel_jobs,\n",
    ")\n",
    "\n",
    "# Start hyperparameter tuning job\n",
    "my_tuner.fit(job_name=tuning_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualize Results\n",
    "\n",
    "To visualize our results, we parse AMT's history to collect all configurations of sub-networks and the corresponding metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = my_tuner.analytics().dataframe()\n",
    "data = []\n",
    "configs = []\n",
    "for i, t in enumerate(my_tuner.analytics().training_job_summaries()):\n",
    "    jn = t[\"TrainingJobName\"]\n",
    "    df = sagemaker.analytics.TrainingJobAnalytics(jn).dataframe()\n",
    "\n",
    "    row = history[history[\"TrainingJobName\"] == jn]\n",
    "    config = {\n",
    "        \"num_heads\": int(row[\"num_heads\"].iloc[0].strip('\"')),\n",
    "        \"num_layers\": int(row[\"num_layers\"]),\n",
    "        \"num_units\": int(row[\"num_units\"]),\n",
    "    }\n",
    "    configs.append(config)\n",
    "\n",
    "    p = []\n",
    "    for j, metric in enumerate(metric_definitions):\n",
    "        metric_name = metric[\"Name\"]\n",
    "        if \"metric_name\" not in df.keys():\n",
    "            continue\n",
    "\n",
    "        if metric_name == \"validation-performance\":\n",
    "            y = 1 - float(df[df[\"metric_name\"] == metric_name][\"value\"])\n",
    "        else:\n",
    "            y = float(df[df[\"metric_name\"] == metric_name][\"value\"])\n",
    "        p.append(y)\n",
    "    if len(p) > 0:\n",
    "        data.append(p)\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now visualize the Pareto set, which represents the optimal set of sub-networks that dominate all other sub-networks in at least one metric. This implies that when we move from one sub-network of the Pareto set to another, we must either sacrifice performance or model size but improve the other.\n",
    "\n",
    "Ultimately, the Pareto set provides us the flexibility to choose the sub-network that best suits our preferences. We can decide how much we want to reduce the size of our network and how much performance we are willing to sacrifice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from multi_objective import get_pareto_optimal\n",
    "\n",
    "idx = get_pareto_optimal(data)\n",
    "x = data[idx, 0]\n",
    "y = data[idx, 1]\n",
    "\n",
    "plt.scatter(\n",
    "    x, y, marker=\"o\", s=80, facecolors=\"none\", edgecolors=\"C0\", linewidth=2, label=\"Pareto front\"\n",
    ")\n",
    "plt.xlabel(\"number of parameters\")\n",
    "plt.ylabel(\"validation error\")\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(linewidth=\"1\", alpha=0.4, which=\"both\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

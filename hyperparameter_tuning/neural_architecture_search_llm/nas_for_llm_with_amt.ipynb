{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Architecture Search for Large Language Models\n",
    "\n",
    "Neural Architecture Search (NAS) is an effective framework to design neural network architectures automatically.\n",
    "This notebook demonstrates how to utilize NAS for compressing a large language model that has been fine-tuned for a specific target task. The main objective is to reduce the model size while maintaining performance as much as possible. To achieve this, we search for sub-networks within the model that jointly optimize the parameter count and validation error.\n",
    "\n",
    "Sub-networks can be defined in various ways. In this context, we consider subsets of multi-head attention and fully-connected layers, with a reduced number of heads in the multi-head attention layer and units in the intermediate layers.\n",
    "\n",
    "Our NAS approach consists of two steps:\n",
    "\n",
    "1. Initial fine-tuning of the pre-trained model on the target task using weight-sharing based NAS training strategies. The pre-trained model serves as a 'super-network' that encompasses a large, finite set of sub-networks. To prevent sub-networks from co-adapting, we modify the fine-tuning process by updating only specific parts of the network (i.e., subsets of all layers) in each update step.\n",
    "\n",
    "2. In the second step, we employ multi-objective search through [SageMaker Automated Model Tuning (AMT)](https://aws.amazon.com/sagemaker/automatic-model-tuning/) to identify a set of sub-networks that offer an optimal trade-off between parameter count and validation error for the target task.\n",
    "\n",
    "Finally, we can visualize the so-called **Pareto set** of architectures that optimally balance between parameter count and validation error, and select the best-suited model that strikes the right balance between model size and validation error for us.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "- Currently, the notebook limited to the BERT model family.\n",
    "- We exclusively focus on supervised fine-tuning using labeled datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U 'sagemaker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset \n",
    "\n",
    "For illustration purposes, we use the [Recognizing Textual Entailment](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) dataset from the [GLUE](https://gluebenchmark.com/) benchmarking suite. For simplicity, we load the dataset via the [dataset library](https://huggingface.co/docs/datasets/index) from HuggingFace inside our training script.\n",
    "The goal of this dataset is to identify whether the meaning of one sentence can be inferred from the other sentence. \n",
    "Example sentence pair:\n",
    " \n",
    "sentence 1:\n",
    "*Only a week after it had no comment on upping the storage capacity of its Hotmail e-mail service, Microsoft early Thursday announced it was boosting the allowance to 250 MB to follow similar moves by rivals such as Google, Yahoo, and Lycos.*\n",
    "\n",
    "sentence 2:\n",
    "*Microsoft's Hotmail has raised its storage capacity to 250 MB.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "hyperparameters = dict()\n",
    "\n",
    "hyperparameters[\"seed\"] = seed\n",
    "hyperparameters[\"output_dir\"] = \"/opt/ml/checkpoints\"\n",
    "hyperparameters[\"task_name\"] = \"rte\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Split Dataset in Training / Validation\n",
    "\n",
    "As mentioned earlier, after fine-tuning our 'super-network' on the training dataset, we proceed with a multi-objective search to identify the optimal set of sub-networks that optimally balance between generalization performance and parameter count. Since we cannot directly compute generalization performance, we approximate it by measuring accuracy on a hold-out validation dataset.\n",
    "\n",
    "To achieve this, we split the original training dataset from GLUE into a training/validation set. The training set is exclusively used for fine-tuning purposes, while the validation data is employed for the subsequent multi-objective search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pre-trained Model\n",
    "\n",
    "We use the [pre-trained BERT model](https://huggingface.co/bert-base-cased) from the HuggingFace hub, which consists of 12 layers with 12 heads in each multi-head attention layers and 3072 units in the fully connected layers. See [Devlin et al.](https://aclanthology.org/N19-1423/) for a more detailed description of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = \"bert-base-cased\"\n",
    "hyperparameters[\"model_name_or_path\"] = model_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train weight-sharing based Super-Network\n",
    "\n",
    "We now fine-tune our pre-training network, i.e. super-network with the following hyperparameters. We have to pass an additional argument to specify if our dataset is regression or not (determines the loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters[\"per_device_train_batch_size\"] = 8\n",
    "hyperparameters[\"per_device_eval_batch_size\"] = 8\n",
    "hyperparameters[\"learning_rate\"] = 2e-05\n",
    "hyperparameters[\"num_train_epochs\"] = 5\n",
    "hyperparameters[\"save_strategy\"] = \"epoch\"\n",
    "hyperparameters[\n",
    "    \"is_regression\"\n",
    "] = False  # set this to True if your dataset is a regression dataset, for example STSB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the checkpoint of the model on S3, such that we can load it later in the second phase for the multi-objective search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = Session()\n",
    "s3_bucket = session.default_bucket()\n",
    "s3_bucket_prefix = \"nas_amt/model_checkpoint\"\n",
    "s3_path = f\"s3://{s3_bucket}/{s3_bucket_prefix}\"\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"epoch\", \"Regex\": \"epoch: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"training-loss\", \"Regex\": \"training loss: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"num-parameters\", \"Regex\": \"number of parameters: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"validation-error\", \"Regex\": \"validation error: ([0-9\\\\.]+)\"},\n",
    "]\n",
    "\n",
    "sm_args = dict(\n",
    "    entry_point=\"training.py\",\n",
    "    source_dir=os.path.abspath(\"\"),\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    instance_count=1,\n",
    "    py_version=\"py39\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    transformers_version=\"4.26\",\n",
    "    max_run=3600 * 72,\n",
    "    role=get_execution_role(),\n",
    "    checkpoint_local_path=\"/opt/ml/checkpoints\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    checkpoint_s3_uri=s3_path,\n",
    "    metric_definitions=metric_definitions,\n",
    ")\n",
    "\n",
    "est = HuggingFace(**sm_args)\n",
    "est.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multi-objective search for sub-networks\n",
    "\n",
    "After the fine-tuning process, we begin the multi-objective search by sampling random sub-networks using AMT. A sub-network is defined by its number of layers, heads, and units in the intermediate fully connected layers. To access a sub-network, we place a binary mask over the super-network and mask out all components (i.e., heads, units) that are not part of the sub-network. Note that, HuggingFace transformers needs the hidden size to be a multiple of the number of head. We cannot change the hidden size, and hence the number of heads has to be in [1, 3, 6, 12].\n",
    "\n",
    "In contrast to single-objective optimization, in the multi-objective setting, we typically do not have a single solution that simultaneously optimizes all objectives. Instead, we aim to collect a set of solutions that *dominate* all other solutions in at least one objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can start the multi-objective search through AMT. We sample a total of 100 random sub-networks (defined by the parameter `max_jobs`) and evaluate 10 networks simultaneously (defined by `max_parallel_jobs`). The code to load the model checkpoint and evaluate the sub-network is available in the `evaluate_subnetwork.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Maximum number of sub-networks we will evaluate\n",
    "max_jobs = 10\n",
    "max_parallel_jobs = 5\n",
    "\n",
    "# Entry point script to load the super-network and evaluate a sub-network\n",
    "entry_point = \"evaluate_subnetwork.py\"\n",
    "\n",
    "# Command line arguments for the entry point script\n",
    "hyperparameters = {\"model_name_or_path\": model_type, \"output_dir\": \"./tmp\", \"task_name\": \"rte\"}\n",
    "\n",
    "# Define the metric we want to minimize\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"num-parameters\", \"Regex\": \"number of parameters: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"validation-error\", \"Regex\": \"validation error: ([0-9\\\\.]+)\"},\n",
    "]\n",
    "\n",
    "# Define HuggingFace estimator\n",
    "estimator = HuggingFace(\n",
    "    entry_point=entry_point,\n",
    "    source_dir=\"./\",\n",
    "    instance_type=\"ml.g4dn.xlarge\",  # instance types for the SageMaker training jobs\n",
    "    instance_count=1,\n",
    "    py_version=\"py39\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    transformers_version=\"4.26\",\n",
    "    max_run=3600 * 72,\n",
    "    role=get_execution_role(),\n",
    "    volume_size=125,\n",
    "    model_uri=s3_path,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "\n",
    "current_time = datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "tuning_job_name = f\"nas-search-{current_time}\"\n",
    "\n",
    "# Search space to define sub-networks\n",
    "hyperparameter_ranges = {\n",
    "    \"num_layers\": IntegerParameter(0, 12),\n",
    "    # To meet HuggingFace constraints, we can only set the number of head to these values\n",
    "    \"num_heads\": CategoricalParameter([1, 3, 6, 12]),\n",
    "    \"num_units\": IntegerParameter(0, 3072),\n",
    "}\n",
    "\n",
    "# Define AMT Tuner object\n",
    "my_tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name=\"validation-error\",\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_jobs=max_jobs,\n",
    "    strategy=\"Random\",\n",
    "    random_seed=seed,\n",
    "    objective_type=\"Minimize\",\n",
    "    max_parallel_jobs=max_parallel_jobs,\n",
    ")\n",
    "\n",
    "# Start hyperparameter tuning job\n",
    "my_tuner.fit(job_name=tuning_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualize Results\n",
    "\n",
    "To visualize our results, we parse AMT's history to collect all configurations of sub-networks and the corresponding metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = my_tuner.analytics().dataframe()\n",
    "data = []\n",
    "configs = []\n",
    "for i, t in enumerate(my_tuner.analytics().training_job_summaries()):\n",
    "    jn = t[\"TrainingJobName\"]\n",
    "    df = sagemaker.analytics.TrainingJobAnalytics(jn).dataframe()\n",
    "\n",
    "    row = history[history[\"TrainingJobName\"] == jn]\n",
    "    config = {\n",
    "        \"num-heads\": int(row[\"num_heads\"].iloc[0].strip('\"')),\n",
    "        \"num-layers\": int(row[\"num_layers\"]),\n",
    "        \"num-units\": int(row[\"num_units\"]),\n",
    "    }\n",
    "    configs.append(config)\n",
    "\n",
    "    p = []\n",
    "    for j, metric in enumerate(metric_definitions):\n",
    "        metric_name = metric[\"Name\"]\n",
    "        if \"metric_name\" not in df.keys():\n",
    "            continue\n",
    "        y = float(df[df[\"metric_name\"] == metric_name][\"value\"])\n",
    "        p.append(y)\n",
    "    if len(p) > 0:\n",
    "        data.append(p)\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now visualize the Pareto set, which represents the optimal set of sub-networks that dominate all other sub-networks in at least one metric. This implies that when we move from one sub-network of the Pareto set to another, we must either sacrifice performance or model size but improve the other.\n",
    "\n",
    "Ultimately, the Pareto set provides us the flexibility to choose the sub-network that best suits our preferences. We can decide how much we want to reduce the size of our network and how much performance we are willing to sacrifice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from multi_objective import get_pareto_optimal\n",
    "\n",
    "\n",
    "# get results of the un-pruned network\n",
    "df = sagemaker.analytics.TrainingJobAnalytics(est.jobs[0].name).dataframe()\n",
    "validation_error_unpruned_network = float(df[df[\"metric_name\"] == \"validation-error\"].value.min())\n",
    "params_unpruned_network = int(df[df[\"metric_name\"] == \"num-parameters\"].value.min())\n",
    "plt.scatter(\n",
    "    params_unpruned_network,\n",
    "    validation_error_unpruned_network,\n",
    "    marker=\"o\",\n",
    "    s=80,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"C3\",\n",
    "    linewidth=2,\n",
    "    label=\"un-pruned super-network\",\n",
    ")\n",
    "\n",
    "# get Pareto optimal points\n",
    "idx = get_pareto_optimal(data)\n",
    "x = data[idx, 0]\n",
    "y = data[idx, 1]\n",
    "\n",
    "plt.scatter(\n",
    "    x,\n",
    "    y,\n",
    "    marker=\"o\",\n",
    "    s=80,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"C0\",\n",
    "    linewidth=2,\n",
    "    label=\"Pareto front (sub-networks)\",\n",
    ")\n",
    "plt.xlabel(\"number of parameters\")\n",
    "plt.ylabel(\"validation error\")\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(linewidth=\"1\", alpha=0.4, which=\"both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy Model\n",
    "\n",
    "Base on the Pareto front above, we can select the best model for our use case and deploy it on an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = np.arange(len(configs))[idx]\n",
    "pareto_optimal_sub_networks = [configs[i] for i in indicies]\n",
    "config_to_deploy = pareto_optimal_sub_networks[-1]  # Let's take the largest model in the Pareto set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=s3_path + \"/model.tar.gz\",\n",
    "    role=get_execution_role(),\n",
    "    transformers_version=\"4.26\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    py_version=\"py39\",\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"./\",\n",
    "    env={\"SM_HPS\": json.dumps(config_to_deploy)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(initial_instance_count=1, instance_type=\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query out new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\": [\n",
    "        (\n",
    "            \"No Weapons of Mass Destruction Found in Iraq Yet\",\n",
    "            \"Weapons of Mass Destruction Found in Iraq.\",\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are done we can delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/hyperparameter_tuning|neural_architecture_search_llm|nas_for_llm_with_amt.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
